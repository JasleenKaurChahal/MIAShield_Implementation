# MIAShield_Implementation

## Summary
The purpose of the paper is to propose MIAShield, which can act as a defense against Membership Inference Attacks, referred to as MIA. MIAs are the common attacks where an adversary can access the samples  used to train the model. These attacks pose a threat to privacy of individuals whose data is being used to train the privacy-sensitive model. An example of MIAs will be using sensitive healthcare data containing a  Cancer Patient’s tumor image, being used  to train a model to classify other individuals’s image into benign or cancerous if accessed by an adversary can cause serious privacy breach.

MIA are generally result of overfitting models and can either be one of the following attacks:
Probability Dependent
Label Dependent

In order to overcome this problem, MIAShield was introduced which works on the principle of “primitive exclusion” of target. The main idea is to divide the training datasets into disjoint datasets and then train the model in such a way that each sample belongs to one of the dataset. This avoids the problem of overfitting and takes into account the exclusion oracle. The existing membership inference attack defense techniques hide the target data sample by various mechanisms like strong regularization, knowledge distillation, confidence masking or differential privacy. MIAShield is based on a principle of Exclusion of member samples rather than masking their presence. 

In MIAShield functioning, firstly the training data set is split into n disjoint subsets. These subsets are used to train the models. Disjoint subsets assure that a target data-point belongs to only one subset. The exclusion oracle eliminates the model that contains the data point given for the prediction. Since the performance or effectiveness of MIAShield is dependent on the accuracy of exclusion oracle the different member exclusion techniques have been proposed namely Model confidence based exclusion, Exact signature based exclusion, Approximate signature based exclusion, Classifier based exclusion. 

In model confidence based exclusion, the most confident model on the most voted prediction gets excluded. This may not always be the case because models would predict a label with high confidence but the label may turn out to be the wrong one. This is especially true when, during training, models pick up spurious correlations instead of truly distinguishing features of a sample. In Exact signature based exclusion, the cryptographic hash function is used to compute the signature of each sample. Though the technique is efficient and faster it has potential limitations like it does not detect members if an adversary slightly modifies inputs (e.g., changes one pixel in an image). Second, it is vulnerable to timing attacks where an adversary carefully observes response-time difference of the prediction API on members and non-members. The third technique is approximate signature based exclusion based on an idea of perceptual hashing where we generate the fingerprint for each of the images. Hence, it resolves the challenge we face in exact signature based exclusion since the similar looking images will be mapped to the similar hash code. 

In CBE the exclusion of the model, the underlying data characteristics are used to identify and exclude the matching model.First of all we will explore the characteristics of each data point D, then we calculate the principle component analysis (PCA) on data point D, to calculate the features of D. After that the model will  calculate the confidence C and label Y by training the model. Confidence C will play a decisive role in training the MIA attack model. PCA in this method is to make the model lightweight so that it will train faster and also it will reduce the probability of overfitting.

The evaluation of the result has been distributed based on five parameters. The exclusion oracle accuracy is computed as the percentage of correctly excluded samples out of total submitted samples.The model test accuracy which is the percentage of test samples correctly predicted by a model.
Generalization gap is the difference between the model accuracy over test samples. Attack area under the curve, which is if higher then the attack is more successful. And lastly the attack advantage which is the maximum difference between true positive rate and the false positive rate. 

## Implementation

First of all, we have read the MIAShield paper and understood all the oracle methods described. Then we ran the provided starter code and trained a multiclass model using the CIFAR-10 dataset. We modified the initial model and added more Convulational and BatchNormalization layers in order to improve the accuracy of the model. After training, the model got an accuracy of 87.48% in the test dataset. Then we implemented the first two oracle methods: Model Confidence Based Exclusion (MCE) and Exact Matching Based Exclusion (ESE). For the MCE method we splitted the train dataset into 5 disjoint subsets and for each subset we applied Data Augmentation to increase the number of samples and then trained an individual model. After that we implemented a MCE function that receives an image as input and then got five predictions from the individual models. The most confidence model is identified and subsequently excluded. The final prediction is calculated using the remaining models. Then we calculated the accuracy of the MCE method. It got an accuracy of 81.94% in the test dataset. This means that the MCE method has almost 6% less accuracy than the original model.

For the ESE method, we used the same 5 individual models but this time we have to determine whether the image to predict is part of the training set in any of the individual models or not. For this, we applied a hash function to each training image and stored the results in a dictionary whose keys are the hash results and its values are the id of the individual model that contains such an image. Then we implemented an ESE function that  receives an image as input and then got five predictions from the individual models. We then verify if the input image belongs to the training dataset by looking into the hash dictionary. If the hash image is in the dictionary, the individual model is identified and excluded. The final prediction is calculated using the remaining models. The ESE method got an accuracy of 83.24% in the test dataset. This means that the ESE method has almost 4% less accuracy than the original model.


